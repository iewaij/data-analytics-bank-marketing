{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-57dda4c5-84f0-40f7-aece-a9aa127ad294",
    "output_cleared": false
   },
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-ffaa2806-9863-42d3-b160-c7ffb11fc495",
    "output_cleared": false
   },
   "source": [
    "### 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-33c5527e-e901-492f-a6b6-762e1a349100",
    "execution_millis": 1,
    "execution_start": 1605020567707,
    "output_cleared": false,
    "source_hash": "115b99d8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, average_precision_score, recall_score, balanced_accuracy_score, roc_auc_score, precision_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-716c671c-438e-4882-b4b5-a47faf981bec",
    "execution_millis": 2,
    "execution_start": 1605020567713,
    "output_cleared": false,
    "source_hash": "4caf264f"
   },
   "outputs": [],
   "source": [
    "def import_dataset(filename):\n",
    "    \"\"\"\n",
    "    Import the dataset from the path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        filename : str\n",
    "            filename with path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        data : DataFrame\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "        bank_mkt = import_dataset(\"../data/BankMarketing.csv\")\n",
    "    \"\"\"\n",
    "    bank_mkt = pd.read_csv(filename,\n",
    "                           na_values=[\"unknown\", \"nonexistent\"],\n",
    "                           true_values=[\"yes\", \"success\"],\n",
    "                           false_values=[\"no\", \"failure\"])\n",
    "    # Treat pdays = 999 as missing values\n",
    "    bank_mkt[\"pdays\"] = bank_mkt[\"pdays\"].replace(999, pd.NA)\n",
    "    # Convert types, \"Int64\" is nullable integer data type in pandas\n",
    "    bank_mkt = bank_mkt.astype(dtype={\"age\": \"Int64\",\n",
    "                                      \"job\": \"category\",\n",
    "                                      \"marital\": \"category\",\n",
    "                                      \"education\": \"category\",\n",
    "                                      \"default\": \"boolean\",\n",
    "                                      \"housing\": \"boolean\",\n",
    "                                      \"loan\": \"boolean\",\n",
    "                                      \"contact\": \"category\",\n",
    "                                      \"month\": \"category\",\n",
    "                                      \"day_of_week\": \"category\",\n",
    "                                      \"duration\": \"Int64\",\n",
    "                                      \"campaign\": \"Int64\",\n",
    "                                      \"pdays\": \"Int64\",\n",
    "                                      \"previous\": \"Int64\",\n",
    "                                      \"poutcome\": \"boolean\",\n",
    "                                      \"y\": \"boolean\"})\n",
    "    # Drop 12 duplicated rows\n",
    "    bank_mkt = bank_mkt.drop_duplicates().reset_index(drop=True)\n",
    "    # reorder categorical data\n",
    "    bank_mkt[\"education\"] = bank_mkt[\"education\"].cat.reorder_categories([\"illiterate\", \"basic.4y\", \"basic.6y\", \"basic.9y\", \"high.school\", \"professional.course\", \"university.degree\"], ordered=True)\n",
    "    bank_mkt[\"month\"] = bank_mkt[\"month\"].cat.reorder_categories([\"mar\", \"apr\", \"jun\", \"jul\", \"may\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"], ordered=True)\n",
    "    bank_mkt[\"day_of_week\"] = bank_mkt[\"day_of_week\"].cat.reorder_categories([\"mon\", \"tue\", \"wed\", \"thu\", \"fri\"], ordered=True)\n",
    "    return bank_mkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-663297ab-ae50-4f6e-853c-90d665823d2c",
    "execution_millis": 20,
    "execution_start": 1605020567720,
    "output_cleared": false,
    "source_hash": "d85a72d5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/BankMarketing.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-02255dedd489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbank_mkt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/BankMarketing.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-d23a640fb18b>\u001b[0m in \u001b[0;36mimport_dataset\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     19\u001b[0m                            \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nonexistent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                            \u001b[0mtrue_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"success\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                            false_values=[\"no\", \"failure\"])\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Treat pdays = 999 as missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mbank_mkt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pdays\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbank_mkt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pdays\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/BankMarketing.csv'"
     ]
    }
   ],
   "source": [
    "bank_mkt = import_dataset(\"../data/BankMarketing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-4382fdab-5693-4497-b8d7-261bc1fb2510",
    "output_cleared": false
   },
   "source": [
    "### 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-c8aa9616-7e75-404b-949a-5ea199d9da67",
    "output_cleared": false,
    "source_hash": "d17e81cc"
   },
   "outputs": [],
   "source": [
    "def benchmark(data, preprocessor, clf):\n",
    "    \"\"\"\n",
    "    Benchmark preprocessor and clf's performance on train, validation and test sets. \n",
    "    All the data transformation should be handled by preprocessor and estimation should be handled by clf.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        data : DataFrame\n",
    "        \n",
    "        preprocessor : Pipeline\n",
    "        \n",
    "        clf : estimator\n",
    "        \n",
    "        name : str, default = None\n",
    "        \n",
    "        compare_to: DataFrame, default = None\n",
    "        \n",
    "    \"\"\"\n",
    "    X_train, y_train, X_test, y_test, X_ttrain, y_ttrain, X_validate, y_validate = split_dataset(data, preprocessor)\n",
    "    X_sets = [X_ttrain, X_validate, X_test]\n",
    "    y_sets = [y_ttrain, y_validate, y_test]\n",
    "    \n",
    "    metric_names = [\"TNR\", \"TPR\", \"bACC\", \"ROC\", \"REC\", \"PRE\", \"AP\"]\n",
    "    set_names = [\"Train\", \"Validate\", \"Test\"]\n",
    "    metric_df = pd.DataFrame(index=metric_names, columns=set_names)\n",
    "    \n",
    "    try:\n",
    "        clf.fit(X_ttrain, y_ttrain, eval_set=(X_validate, y_validate), verbose=False)\n",
    "    except (ValueError, TypeError):\n",
    "        clf.fit(X_ttrain, y_ttrain)\n",
    "        \n",
    "    for name, X, y in zip(set_names, X_sets, y_sets):\n",
    "        # Re-fit model on train set before test set evaluation except CatBoost\n",
    "        if name == \"Test\":\n",
    "            try:\n",
    "                clf.fit(X_ttrain, y_ttrain, eval_set=(X_validate, y_validate), verbose=False)\n",
    "            except (ValueError, TypeError):\n",
    "                clf.fit(X_train, y_train)\n",
    "                \n",
    "        y_pred = clf.predict(X)\n",
    "        \n",
    "        try:\n",
    "            y_score = clf.decision_function(X)\n",
    "        except AttributeError:\n",
    "            y_score = clf.predict_proba(X)[:, 1]\n",
    "            \n",
    "        metrics = [recall_score(y, y_pred, pos_label=0),\n",
    "                   recall_score(y, y_pred),\n",
    "                   balanced_accuracy_score(y, y_pred),\n",
    "                   roc_auc_score(y, y_score),\n",
    "                   recall_score(y, y_pred),\n",
    "                   precision_score(y, y_pred),\n",
    "                   average_precision_score(y, y_score)]\n",
    "        metric_df[name] = metrics\n",
    "        \n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-e155f634-d97b-4c43-aeda-9a3cc0a085a9",
    "output_cleared": false,
    "source_hash": "ea975b76"
   },
   "outputs": [],
   "source": [
    "#### Define plot_confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False,\n",
    "                          title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]),range(cm.shape[1])): \n",
    "         plt.text(j, i, format(cm[i, j], fmt), \n",
    "                  horizontalalignment=\"center\", fontsize=16,                  \n",
    "                  color=\"white\" if cm[i, j] > thresh else \"black\") \n",
    "    plt.ylabel('True label')     \n",
    "    plt.xlabel('Predicted label')     \n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-70ea2452-37b1-46ad-8969-64061ea38a47",
    "output_cleared": false,
    "source_hash": "21867309"
   },
   "outputs": [],
   "source": [
    "def split_dataset(data, preprocessor, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset into train, test and validation sets using preprocessor.\n",
    "    Because the random state of validation set is not specified, the validation set will be different each time when the function is called.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        data : DataFrame\n",
    "\n",
    "        preprocessor : Pipeline\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        datasets : tuple\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "        from sklearn.preprocessing import OrdinalEncoder\n",
    "        data = import_dataset(\"../data/BankMarketing.csv\").interpolate(method=\"pad\").loc[:, [\"job\", \"education\", \"y\"]]\n",
    "        # To unpack all train, test, and validation sets \n",
    "        X_train, y_train, X_test, y_test, X_ttrain, y_ttrain, X_validate, y_validate = split_dataset(data, OrdinalEncoder())\n",
    "        # To unpack train and test sets.\n",
    "        X_train, y_train, X_test, y_test, *other_sets = split_dataset(data, OrdinalEncoder())\n",
    "        # To unpack test and validation set\n",
    "        *other_sets, X_test, y_test, X_ttrain, y_ttrain, X_validate, y_validate = split_dataset(data, OrdinalEncoder())\n",
    "        # To unpack only train set.\n",
    "        X_train, y_train, *other_sets = split_dataset(data, OneHotEncoder())\n",
    "    \"\"\"\n",
    "    train_test_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "    for train_index, test_index in train_test_split.split(data.drop(\"y\", axis=1), data[\"y\"]):\n",
    "        train_set = data.loc[train_index].reset_index(drop=True)\n",
    "        test_set = data.loc[test_index].reset_index(drop=True)\n",
    "\n",
    "    y_train = train_set[\"y\"].astype(\"int\").to_numpy()\n",
    "    y_test = test_set[\"y\"].astype(\"int\").to_numpy()\n",
    "    X_train = preprocessor.fit_transform(train_set, y_train)\n",
    "    X_test = preprocessor.transform(test_set)\n",
    "        \n",
    "    train_validate_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "    for ttrain_index, validate_index in train_validate_split.split(X_train, y_train):\n",
    "        ttrain_set = train_set.loc[ttrain_index].reset_index(drop=True)\n",
    "        validate_set = train_set.loc[validate_index].reset_index(drop=True)\n",
    "    \n",
    "    y_ttrain = ttrain_set[\"y\"].astype(\"int\").to_numpy()\n",
    "    y_validate = validate_set[\"y\"].astype(\"int\").to_numpy()\n",
    "    X_ttrain = preprocessor.fit_transform(ttrain_set, y_ttrain)\n",
    "    X_validate = preprocessor.transform(validate_set)\n",
    "    \n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train = X_train.to_numpy()\n",
    "        X_test = X_test.to_numpy()\n",
    "        X_ttrain = X_ttrain.to_numpy()\n",
    "        X_validate = X_validate.to_numpy()\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test, X_ttrain, y_ttrain, X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-5847ddd4-1d7a-4849-82c3-5c32037c85ec",
    "output_cleared": false,
    "source_hash": "e952080"
   },
   "outputs": [],
   "source": [
    "def cat_encode(X,\n",
    "               drop=[\"duration\", \"y\"],\n",
    "               cut=None,\n",
    "               cyclic=None,\n",
    "               target=None):\n",
    "    \"\"\"\n",
    "    Encode and transform categorical data into numerical values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        X : DataFrame\n",
    "        \n",
    "        drop : list, default = [\"duration\", \"y\"]\n",
    "        \n",
    "        cut : list\n",
    "        \n",
    "        cyclic : list\n",
    "        \n",
    "        target : list\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        X : DataFrame\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    bank_mkt = import_dataset(\"../data/BankMarketing.csv\")\n",
    "    X = cat_encode(bank_mkt)\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    \n",
    "    # `month` will be encoded to the corresponding number, e.g. \"mar\" -> 3.\n",
    "    month_map = {\"mar\": 3,\n",
    "                 \"apr\": 4,\n",
    "                 \"may\": 5,\n",
    "                 \"jun\": 6,\n",
    "                 \"jul\": 7,\n",
    "                 \"aug\": 8,\n",
    "                 \"sep\": 9,\n",
    "                 \"oct\": 10,\n",
    "                 \"nov\": 11,\n",
    "                 \"dec\": 12}\n",
    "    X[\"month\"] = X[\"month\"].replace(month_map).astype(\"Int64\")\n",
    "    \n",
    "    if cut != None:\n",
    "        if \"pdays\" in cut:\n",
    "            X[\"pdays\"] = X[\"pdays\"].fillna(-1)\n",
    "            # Clients who have been contacted but do not have pdays record\n",
    "            X.loc[X[\"pdays\"].isna() & X[\"poutcome\"].notna(), \"pdays\"] = 999\n",
    "            # Cut pdays into categories\n",
    "            X[\"pdays\"] = pd.cut(X[\"pdays\"], [0, 3, 5, 10, 15, 30, 1000], labels=[3, 5, 10, 15, 30, 1000], include_lowest=True).astype(\"Int64\")\n",
    "    else:\n",
    "        # Fill missing values in pdays as 999\n",
    "        X[\"pdays\"] = X[\"pdays\"].fillna(999)\n",
    "    \n",
    "    if cyclic != None:\n",
    "        if \"month\" in cyclic:\n",
    "            X['month_sin'] = np.sin(2 * np.pi * X[\"month\"]/12)\n",
    "            X['month_cos'] = np.cos(2 * np.pi * X[\"month\"]/12)\n",
    "            X = X.drop(\"month\", axis=1)\n",
    "        if \"day_of_week\" in cyclic:\n",
    "            X[\"day_of_week\"] = X[\"day_of_week\"].cat.codes\n",
    "            X['day_sin'] = np.sin(2 * np.pi * X[\"day_of_week\"]/5)\n",
    "            X['day_cos'] = np.cos(2 * np.pi * X[\"day_of_week\"]/5)\n",
    "            X = X.drop(\"day_of_week\", axis=1)\n",
    "    \n",
    "    # Transform target encoded feature as str\n",
    "    if target != None:\n",
    "        X[target] = X[target].astype(\"str\")\n",
    "        \n",
    "    # Drop features\n",
    "    X = X.drop(drop, axis=1)\n",
    "    \n",
    "    # Other categorical features will be coded as its order in pandas categorical index\n",
    "    X = X.apply(lambda x: x.cat.codes if pd.api.types.is_categorical_dtype(x) else (x.astype(\"Int64\") if pd.api.types.is_bool_dtype(x) else x))\n",
    "    \n",
    "    # Fill missing values as -1\n",
    "    X = X.fillna(-1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00010-b0b12ec4-a1fb-4256-b640-1c2fc6b37863",
    "output_cleared": false,
    "source_hash": "6a85ef8b"
   },
   "outputs": [],
   "source": [
    "#data preparation with freq_imputer, and split train & test set\n",
    "cat_encoder = FunctionTransformer(cat_encode)\n",
    "\n",
    "freq_features = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\"]\n",
    "\n",
    "freq_imputer = ColumnTransformer([\n",
    "    (\"freq_imputer\", SimpleImputer(missing_values=-1, strategy=\"most_frequent\"), freq_features)], \n",
    "      remainder=\"passthrough\")\n",
    "\n",
    "freq_encoder = make_pipeline(cat_encoder, freq_imputer)\n",
    "\n",
    "X_train, y_train, X_test, y_test, *other_sets = split_dataset(bank_mkt, freq_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-afc7ef68-8bc0-417c-ad12-db36036cf306",
    "output_cleared": false
   },
   "source": [
    "## 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-527b38c6-7f13-4c92-bd03-a812c2e3a5c3",
    "output_cleared": false
   },
   "source": [
    "#### Logistic Model\n",
    "![image.png](attachment:image.png)\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00013-f0d035bf-a21b-494e-aa24-7271fdae3dae",
    "output_cleared": false,
    "source_hash": "7f76f59e"
   },
   "outputs": [],
   "source": [
    "#### import LogisticRegression and fit train set data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lrmodel = LogisticRegression(class_weight='balanced',max_iter=10000) \n",
    "lrmodel.fit(X_train, y_train)\n",
    "y_train_pred = lrmodel.predict(X_train)\n",
    "#### model measures for training data\n",
    "cmtr = confusion_matrix(y_train, y_train_pred)\n",
    "acctr = accuracy_score(y_train, y_train_pred)\n",
    "aps_train = average_precision_score(y_train, y_train_pred)\n",
    "\n",
    "#### fit test set data\n",
    "lrmodel.fit(X_test, y_test)\n",
    "y_test_pred = lrmodel.predict(X_test)\n",
    "#### model measures for testing data\n",
    "cmte = confusion_matrix(y_test, y_test_pred)\n",
    "accte = accuracy_score(y_test, y_test_pred)\n",
    "aps_test = average_precision_score(y_test, y_test_pred)\n",
    "\n",
    "print('Accuracy Score:',acctr, '    APS:',aps_train)\n",
    "print('Accuracy Score:',accte, '    APS:',aps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00014-be9395e0-83fb-4257-95ec-5f2be1643ee6",
    "output_cleared": false,
    "scrolled": true,
    "source_hash": "b57966d4"
   },
   "outputs": [],
   "source": [
    "#### Plot confusion matrix for train-prediction\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = ['no', 'yes']\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cmtr, classes=class_names,\n",
    "                      title='Confusion matrix Logistic train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-ea6fcfe8-5e5c-406e-a24a-e99bdb64a6cb",
    "output_cleared": false,
    "source_hash": "4f024e1a"
   },
   "outputs": [],
   "source": [
    "#### Plot confusion matrix for test-prediction\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = ['no', 'yes']\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cmte, classes=class_names,\n",
    "                      title='Confusion matrix Logistic train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00016-6f2eea7f-7901-42c2-99de-840995ed4d83",
    "output_cleared": false,
    "source_hash": "e96c38f1"
   },
   "outputs": [],
   "source": [
    "#### Plot ROC & AUC\n",
    "y_pred_proba = lrmodel.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.title('ROC Curve of Logistic Regression') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-680db018-1b8b-4686-83de-26326c3b2501",
    "output_cleared": false,
    "scrolled": true,
    "source_hash": "8e467007"
   },
   "outputs": [],
   "source": [
    "benchmark(bank_mkt, freq_encoder, lrmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-107801fe-5d92-481c-8415-d2060bdd5379",
    "output_cleared": false
   },
   "source": [
    "### 4.  Grid Search \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00019-0352bf0b-2556-4bdc-8fdf-701bb1d9a0ea",
    "output_cleared": false,
    "scrolled": true,
    "source_hash": "83e8650"
   },
   "outputs": [],
   "source": [
    "#### Try the 1st GridSearch param_grid combination:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lrmodel = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
    "\n",
    "#### Grid Search\n",
    "param_grid = {'penalty': ['l2'],\n",
    "              'C':[0.001,.009,0.01,.09,1,5,10,25,50,100]}\n",
    "GS_lrmodel_1 = GridSearchCV(lrmodel, param_grid, scoring='recall')\n",
    "GS_lrmodel_1.fit(X_train, y_train)\n",
    "lrmodel_gs1 = lrmodel.set_params(**GS_lrmodel_1.best_params_)\n",
    "\n",
    "#### use calibrated model on train set\n",
    "lrmodel_gs1.fit(X_train, y_train)\n",
    "y_train_pred = lrmodel_gs1.predict(X_train)\n",
    "cmtr_gs1 = confusion_matrix(y_train, y_train_pred)\n",
    "acctr_gs1 = accuracy_score(y_train, y_train_pred)\n",
    "aps_train_gs1 = average_precision_score(y_train, y_train_pred)\n",
    "\n",
    "#### test the model\n",
    "lrmodel_gs1.fit(X_test, y_test)\n",
    "y_test_pred = lrmodel_gs1.predict(X_test)\n",
    "cmte_gs1 = confusion_matrix(y_test, y_test_pred)\n",
    "accte_gs1 = accuracy_score(y_test, y_test_pred)\n",
    "aps_test_gs1 = average_precision_score(y_test, y_test_pred)\n",
    "\n",
    "print('Confusion Matrix:\\n',cmtr_gs1,'\\nAccuracy Score:\\n',acctr_gs1, '\\nAPS:\\n',aps_train_gs1)\n",
    "print('Confusion Matrix:\\n',cmte_gs1,'\\nAccuracy Score:\\n',accte_gs1, '\\nAPS:\\n',aps_test_gs1)\n",
    "print('best parameters:',GS_lrmodel_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00020-68ba9ed0-4af4-4aa0-b830-deccd16e55e1",
    "output_cleared": false,
    "source_hash": "4b107610"
   },
   "outputs": [],
   "source": [
    "benchmark(bank_mkt, freq_encoder, lrmodel_gs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00021-9e3e438f-1e23-4d57-816b-06478ffd1832",
    "output_cleared": false,
    "scrolled": true,
    "source_hash": "59242ad9"
   },
   "outputs": [],
   "source": [
    "#### Try the 2nd GridSearch param_grid combination:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lrmodel_gs = LogisticRegression(class_weight='balanced',max_iter=10000)\n",
    "\n",
    "#### Grid Search\n",
    "param_grid = {\"C\":np.logspace(-2,2,7), \n",
    "              \"penalty\":[\"l1\",\"l2\"],\n",
    "              \"solver\": [\"saga\"]}\n",
    "GS_lrmodel_2 = GridSearchCV(lrmodel_gs, param_grid, cv=10)\n",
    "GS_lrmodel_2.fit(X_train, y_train)\n",
    "lrmodel_gs2 = lrmodel_gs.set_params(**GS_lrmodel_2.best_params_)\n",
    "\n",
    "#### use calibrated model on train set\n",
    "lrmodel_gs2.fit(X_train, y_train)\n",
    "y_train_pred = lrmodel_gs2.predict(X_train)\n",
    "cmtr_gs2 = confusion_matrix(y_train, y_train_pred)\n",
    "acctr_gs2 = accuracy_score(y_train, y_train_pred)\n",
    "aps_train_gs2 = average_precision_score(y_train, y_train_pred)\n",
    "#### test the model\n",
    "lrmodel_gs2.fit(X_test, y_test)\n",
    "y_test_pred = lrmodel_gs2.predict(X_test)\n",
    "cmte_gs2 = confusion_matrix(y_test, y_test_pred)\n",
    "accte_gs2 = accuracy_score(y_test, y_test_pred)\n",
    "aps_test_gs2 = average_precision_score(y_test, y_test_pred)\n",
    "\n",
    "print('Confusion Matrix:\\n',cmtr_gs2,'\\nAccuracy Score:\\n',acctr_gs2, '\\nAPS:\\n',aps_train_gs2)\n",
    "print('Confusion Matrix:\\n',cmte_gs2,'\\nAccuracy Score:\\n',accte_gs2, '\\nAPS:\\n',aps_test_gs2)\n",
    "print('best parameters:',GS_lrmodel_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00022-1729a7ee-c99b-4c4f-957c-d48f08f06ec0",
    "output_cleared": false,
    "source_hash": "2c3dc1fd"
   },
   "outputs": [],
   "source": [
    "benchmark(bank_mkt, freq_encoder, lrmodel_gs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00023-c542ab36-0381-4077-bd86-95db6b5329d4",
    "output_cleared": false
   },
   "source": [
    "### 5.  Describe Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00024-e1c632cf-345b-48c3-a4e4-eeb91ff6a980",
    "output_cleared": false,
    "source_hash": "76ecd2d6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def import_dataset(filename):\n",
    "    bank_mkt = pd.read_csv(filename,\n",
    "                           na_values=[\"unknown\", \"nonexistent\"],\n",
    "                           true_values=[\"yes\", \"success\"],\n",
    "                           false_values=[\"no\", \"failure\"])\n",
    "    \n",
    "    bank_mkt[\"pdays\"] = bank_mkt[\"pdays\"].replace(999, pd.NA)\n",
    "    bank_mkt = bank_mkt.astype(dtype={\"age\": \"Int64\",\n",
    "                                      \"job\": \"category\",\n",
    "                                      \"marital\": \"category\",\n",
    "                                      \"education\": \"category\",\n",
    "                                      \"default\": \"boolean\",\n",
    "                                      \"housing\": \"boolean\",\n",
    "                                      \"loan\": \"boolean\",\n",
    "                                      \"contact\": \"category\",\n",
    "                                      \"month\": \"category\",\n",
    "                                      \"day_of_week\": \"category\",\n",
    "                                      \"duration\": \"Int64\",\n",
    "                                      \"campaign\": \"Int64\",\n",
    "                                      \"pdays\": \"Int64\",\n",
    "                                      \"previous\": \"Int64\",\n",
    "                                      \"poutcome\": \"boolean\",\n",
    "                                      \"y\": \"boolean\"})\n",
    "    \n",
    "    bank_mkt[\"education\"] = bank_mkt[\"education\"].cat.reorder_categories([\"illiterate\", \"basic.4y\", \"basic.6y\", \"basic.9y\", \"high.school\", \"professional.course\", \"university.degree\"], ordered=True)\n",
    "    bank_mkt[\"month\"] = bank_mkt[\"month\"].cat.reorder_categories([\"mar\", \"apr\", \"jun\", \"jul\", \"may\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"], ordered=True)\n",
    "    bank_mkt[\"day_of_week\"] = bank_mkt[\"day_of_week\"].cat.reorder_categories([\"mon\", \"tue\", \"wed\", \"thu\", \"fri\"], ordered=True)\n",
    "    return bank_mkt\n",
    "\n",
    "bank_mkt = import_dataset(\"/Users/fanjia_imac27/Desktop/#Python Practice#/【IDAB - Group】/4-【Jiawei】/20201031 - 操作/data/BankMarketing.csv\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit  \n",
    "\n",
    "train_test_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in train_test_split.split(bank_mkt.drop(\"y\", axis=1), bank_mkt[\"y\"]):\n",
    "    bank_train_set = bank_mkt.loc[train_index].reset_index(drop=True)\n",
    "    bank_test_set = bank_mkt.loc[test_index].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "def cat_encode(X, education=None, pdays=None):   \n",
    "    X = X.copy()\n",
    "\n",
    "    if pdays == \"cut\":\n",
    "        X.loc[X[\"pdays\"].isna() & X[\"poutcome\"].notna(), \"pdays\"] = 999\n",
    "        X[\"pdays\"] = pd.cut(X[\"pdays\"], [0, 5, 10, 15, 30, 1000], labels=[1, 2, 3, 4, 5], include_lowest=True).astype(\"Int64\")\n",
    "    \n",
    "    month_map = {\"mar\": 3, \n",
    "                 \"apr\": 4, \n",
    "                 \"jun\": 5, \n",
    "                 \"jul\": 6, \n",
    "                 \"may\": 7, \n",
    "                 \"aug\": 8, \n",
    "                 \"sep\": 9, \n",
    "                 \"oct\": 10, \n",
    "                 \"nov\": 11, \n",
    "                 \"dec\": 12}\n",
    "    X[\"month\"] = X[\"month\"].replace(month_map).astype(\"int\")\n",
    "    \n",
    "    cat_features = [\"job\", \"education\", \"marital\", \"contact\", \"day_of_week\"]\n",
    "    bool_features = [\"default\", \"housing\", \"loan\", \"poutcome\"]\n",
    "    X[cat_features] = X[cat_features].apply(lambda x: x.cat.codes).astype(\"Int64\")\n",
    "    X[bool_features] = X[bool_features].astype(\"Int64\")\n",
    "    \n",
    "    X = X.fillna(-1)\n",
    "    \n",
    "    if education == \"year\":\n",
    "        education_map = { 0: 0, # illiterate\n",
    "                          1: 4, # basic.4y\n",
    "                          2: 6, # basic.6y\n",
    "                          3: 9, # basic.9y\n",
    "                          4: 12, # high.school\n",
    "                          5: 15, # professional course\n",
    "                          6: 16} # university\n",
    "        X[\"education\"] = X[\"education\"].replace(education_map)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "cat_features = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"pdays\", \"poutcome\"]\n",
    "\n",
    "freq_nom_features = [\"job\", \"marital\", \"default\", \"housing\", \"loan\"]\n",
    "\n",
    "freq_ord_features = [\"education\"]\n",
    "\n",
    "one_hot_features = [i for i in cat_features if i not in freq_nom_features and i not in freq_ord_features]\n",
    "\n",
    "cut_encoder = FunctionTransformer(cat_encode, kw_args={\"pdays\":\"cut\"})\n",
    "\n",
    "freq_nom_imputer = Pipeline([\n",
    "    (\"freq_imputer\", SimpleImputer(missing_values=-1, strategy=\"most_frequent\")),\n",
    "    (\"one_hot_encoder\", OneHotEncoder(drop=\"first\", sparse=False))\n",
    "])\n",
    "\n",
    "freq_ord_imputer = Pipeline([\n",
    "    (\"freq_imputer\", SimpleImputer(missing_values=-1, strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "freq_transformer = ColumnTransformer([\n",
    "    (\"freq_nom_imputer\", freq_nom_imputer, freq_nom_features),\n",
    "    (\"freq_ord_imputer\", freq_ord_imputer, freq_ord_features),\n",
    "    (\"one_hot_encoder\", OneHotEncoder(drop=\"first\", sparse=False), one_hot_features)],\n",
    "    remainder=\"passthrough\")  \n",
    "\n",
    "freq_preprocessor = Pipeline([\n",
    "    (\"cut_encoder\", cut_encoder),\n",
    "    (\"freq_transformer\", freq_transformer),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "freq_preprocessor\n",
    "\n",
    "X_train = bank_train_set.drop([\"duration\", \"y\"], axis=1)\n",
    "y_train = bank_train_set[\"y\"].astype(\"int\").to_numpy()\n",
    "\n",
    "X_train_freq = freq_preprocessor.fit_transform(X_train)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-e5e802a9-db45-4a52-a059-0abb5717cbd5",
    "output_cleared": false,
    "source_hash": "179cce29"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y_train,X_train_freq)\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9a02110e-a128-4204-baf9-51cb7cbf3a79",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
