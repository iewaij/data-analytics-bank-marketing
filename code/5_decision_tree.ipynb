{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-053f7d3f-467a-4af7-9ddb-b0fdf3cf6cf7",
    "tags": []
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.rcParams[\"figure.figsize\"] = (6.4, 4.8)\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"figure.titleweight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.titleweight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.titlepad\"] = 10.0\n",
    "plt.rcParams[\"axes.titlelocation\"] = \"left\"\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats(\"svg\")\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, balanced_accuracy_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(filename):\n",
    "    bank_mkt = pd.read_csv(filename,\n",
    "                           na_values=[\"unknown\", \"nonexistent\"],\n",
    "                           true_values=[\"yes\", \"success\"],\n",
    "                           false_values=[\"no\", \"failure\"])\n",
    "    # Treat pdays = 999 as missing values\n",
    "    bank_mkt[\"pdays\"] = bank_mkt[\"pdays\"].replace(999, pd.NA)\n",
    "    # Convert types, \"Int64\" is nullable integer data type in pandas\n",
    "    bank_mkt = bank_mkt.astype(dtype={\"age\": \"Int64\",\n",
    "                                      \"job\": \"category\",\n",
    "                                      \"marital\": \"category\",\n",
    "                                      \"education\": \"category\",\n",
    "                                      \"default\": \"boolean\",\n",
    "                                      \"housing\": \"boolean\",\n",
    "                                      \"loan\": \"boolean\",\n",
    "                                      \"contact\": \"category\",\n",
    "                                      \"month\": \"category\",\n",
    "                                      \"day_of_week\": \"category\",\n",
    "                                      \"duration\": \"Int64\",\n",
    "                                      \"campaign\": \"Int64\",\n",
    "                                      \"pdays\": \"Int64\",\n",
    "                                      \"previous\": \"Int64\",\n",
    "                                      \"poutcome\": \"boolean\",\n",
    "                                      \"y\": \"boolean\"})\n",
    "    # Drop duplicates\n",
    "    bank_mkt = bank_mkt.drop_duplicates().reset_index(drop=True)\n",
    "    # reorder categorical data\n",
    "    bank_mkt[\"education\"] = bank_mkt[\"education\"].cat.reorder_categories([\"illiterate\", \"basic.4y\", \"basic.6y\", \"basic.9y\", \"high.school\", \"professional.course\", \"university.degree\"], ordered=True)\n",
    "    bank_mkt[\"month\"] = bank_mkt[\"month\"].cat.reorder_categories([\"mar\", \"apr\", \"jun\", \"jul\", \"may\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"], ordered=True)\n",
    "    bank_mkt[\"day_of_week\"] = bank_mkt[\"day_of_week\"].cat.reorder_categories([\"mon\", \"tue\", \"wed\", \"thu\", \"fri\"], ordered=True)\n",
    "    return bank_mkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_encode(X):\n",
    "    \"\"\"\n",
    "    Encode categorical data into numerical values.\n",
    "    pdays column will be feature engineered and discretized.\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    # pdays column will be feature engineered and discretized.\n",
    "    X.loc[X[\"pdays\"].isna() & X[\"poutcome\"].notna(), \"pdays\"] = 999\n",
    "    X[\"pdays\"] = pd.cut(X[\"pdays\"], [0, 5, 10, 15, 30, 1000], labels=[1, 2, 3, 4, 5], include_lowest=True).astype(\"Int64\")\n",
    "    # Encode nominal and ordinal features\n",
    "    # `month` will be encoded to the corresponding number, e.g. \"mar\" -> 3.\n",
    "    month_map = {\"mar\": 3,\n",
    "                 \"apr\": 4,\n",
    "                 \"jun\": 5,\n",
    "                 \"jul\": 6,\n",
    "                 \"may\": 7,\n",
    "                 \"aug\": 8,\n",
    "                 \"sep\": 9,\n",
    "                 \"oct\": 10,\n",
    "                 \"nov\": 11,\n",
    "                 \"dec\": 12}\n",
    "    X[\"month\"] = X[\"month\"].replace(month_map).astype(\"int\")\n",
    "    # Other categorical features will be coded as its order in pandas categorical index\n",
    "    cat_features = [\"job\", \"education\", \"marital\", \"contact\", \"day_of_week\"]\n",
    "    bool_features = [\"default\", \"housing\", \"loan\", \"poutcome\"]\n",
    "    X[cat_features] = X[cat_features].apply(lambda x: x.cat.codes).astype(\"Int64\")\n",
    "    X[bool_features] = X[bool_features].astype(\"Int64\")\n",
    "    # Fill missing values as -1\n",
    "    X = X.fillna(-1)\n",
    "    return X\n",
    "\n",
    "tree_encoder = FunctionTransformer(tree_encode)\n",
    "\n",
    "# Features with missing values that should be imputed with most freq value\n",
    "freq_features = [\"job\", \"marital\", \"default\", \"housing\", \"loan\"]\n",
    "\n",
    "# tree_imputer will impute missing values in columns specified by freq_features\n",
    "tree_imputer = ColumnTransformer([\n",
    "    (\"freq_imputer\",\n",
    "     SimpleImputer(missing_values=-1,strategy=\"most_frequent\"),\n",
    "     freq_features)],\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "# Wrap tree_encoder and tree_imputer in one pipeline\n",
    "tree_preprocessor = Pipeline([\n",
    "    (\"basic_encoder\", tree_encoder),\n",
    "    (\"tree_imputer\", tree_imputer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_mkt = import_dataset(\"../data/BankMarketing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in train_test_split.split(bank_mkt.drop(\"y\", axis=1), bank_mkt[\"y\"]):\n",
    "    bank_train_set = bank_mkt.loc[train_index].reset_index(drop=True)\n",
    "    bank_test_set = bank_mkt.loc[test_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tree_preprocessor.fit_transform(bank_train_set.drop([\"duration\", \"y\"], axis=1))\n",
    "y_train = bank_train_set[\"y\"].astype(\"int\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-026bc890-d956-41e0-827d-1d80d6e6cdac",
    "tags": []
   },
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters found: {'criterion': 'entropy', 'max_depth': 1000, 'max_leaf_nodes': 100}, with mean test score: 0.4145968536793858\n",
      "mean test score: 0.3916031948721278, mean train score: 0.3916031948721278, for {'criterion': 'gini', 'max_depth': 1000, 'max_leaf_nodes': 100}.\n",
      "mean test score: 0.3842473456380194, mean train score: 0.3842473456380194, for {'criterion': 'gini', 'max_depth': 1000, 'max_leaf_nodes': 10}.\n",
      "mean test score: nan, mean train score: nan, for {'criterion': 'gini', 'max_depth': 1000, 'max_leaf_nodes': 1}.\n",
      "mean test score: 0.390876033134852, mean train score: 0.390876033134852, for {'criterion': 'gini', 'max_depth': 100, 'max_leaf_nodes': 100}.\n",
      "mean test score: 0.3842473456380194, mean train score: 0.3842473456380194, for {'criterion': 'gini', 'max_depth': 100, 'max_leaf_nodes': 10}.\n",
      "mean test score: nan, mean train score: nan, for {'criterion': 'gini', 'max_depth': 100, 'max_leaf_nodes': 1}.\n",
      "mean test score: 0.3921524588327335, mean train score: 0.3921524588327335, for {'criterion': 'gini', 'max_depth': 10, 'max_leaf_nodes': 100}.\n",
      "mean test score: 0.3842473456380194, mean train score: 0.3842473456380194, for {'criterion': 'gini', 'max_depth': 10, 'max_leaf_nodes': 10}.\n",
      "mean test score: nan, mean train score: nan, for {'criterion': 'gini', 'max_depth': 10, 'max_leaf_nodes': 1}.\n",
      "mean test score: 0.26829524673843685, mean train score: 0.26829524673843685, for {'criterion': 'gini', 'max_depth': 1, 'max_leaf_nodes': 100}.\n",
      "mean test score: 0.26829524673843685, mean train score: 0.26829524673843685, for {'criterion': 'gini', 'max_depth': 1, 'max_leaf_nodes': 10}.\n",
      "mean test score: nan, mean train score: nan, for {'criterion': 'gini', 'max_depth': 1, 'max_leaf_nodes': 1}.\n",
      "mean test score: 0.4145968536793858, mean train score: 0.4145968536793858, for {'criterion': 'entropy', 'max_depth': 1000, 'max_leaf_nodes': 100}.\n",
      "mean test score: 0.38567397878339643, mean train score: 0.38567397878339643, for {'criterion': 'entropy', 'max_depth': 1000, 'max_leaf_nodes': 10}.\n",
      "mean test score: nan, mean train score: nan, for {'criterion': 'entropy', 'max_depth': 1000, 'max_leaf_nodes': 1}.\n",
      "mean test score: 0.4145968536793858, mean train score: 0.4145968536793858, for {'criterion': 'entropy', 'max_depth': 100, 'max_leaf_nodes': 100}.\n",
      "mean test score: 0.38567397878339643, mean train score: 0.38567397878339643, for {'criterion': 'entropy', 'max_depth': 100, 'max_leaf_nodes': 10}.\n",
      "mean test score: nan, mean train score: nan, for {'criterion': 'entropy', 'max_depth': 100, 'max_leaf_nodes': 1}.\n",
      "mean test score: 0.40905723905803537, mean train score: 0.40905723905803537, for {'criterion': 'entropy', 'max_depth': 10, 'max_leaf_nodes': 100}.\n",
      "mean test score: 0.38567397878339643, mean train score: 0.38567397878339643, for {'criterion': 'entropy', 'max_depth': 10, 'max_leaf_nodes': 10}.\n",
      "mean test score: nan, mean train score: nan, for {'criterion': 'entropy', 'max_depth': 10, 'max_leaf_nodes': 1}.\n",
      "mean test score: 0.26829524673843685, mean train score: 0.26829524673843685, for {'criterion': 'entropy', 'max_depth': 1, 'max_leaf_nodes': 100}.\n",
      "mean test score: 0.26829524673843685, mean train score: 0.26829524673843685, for {'criterion': 'entropy', 'max_depth': 1, 'max_leaf_nodes': 10}.\n",
      "mean test score: nan, mean train score: nan, for {'criterion': 'entropy', 'max_depth': 1, 'max_leaf_nodes': 1}.\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "param_grid = [\n",
    "    {\"criterion\": [\"gini\", \"entropy\"],\n",
    "     \"max_depth\": [1000, 100, 10, 1],\n",
    "     \"max_leaf_nodes\": [100, 10, 1]}\n",
    "    ]\n",
    "grid_search = GridSearchCV(decision_tree,\n",
    "                           param_grid,\n",
    "                           scoring=\"average_precision\",\n",
    "                           return_train_score=True,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1)\n",
    "grid_fit = grid_search.fit(X_train, y_train)\n",
    "grid_results = grid_search.cv_results_\n",
    "grid_best_params = grid_search.best_params_\n",
    "\n",
    "grid_fit = grid_search.fit(X_train, y_train)\n",
    "grid_results = grid_search.cv_results_\n",
    "grid_best_params = grid_search.best_params_\n",
    "grid_best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"best parameters found: {grid_best_params}, with mean test score: {grid_best_score}\")\n",
    "\n",
    "for test_score, train_score, params in zip(grid_results[\"mean_test_score\"],\n",
    "                                           grid_results[\"mean_test_score\"],\n",
    "                                           grid_results[\"params\"]):\n",
    "    print(f\"mean test score: {test_score}, mean train score: {train_score}, for {params}.\")"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "f64e53ca-1b5a-45ec-96fc-21aa4b4e83c8",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
