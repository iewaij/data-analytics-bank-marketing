
@misc{noauthor_machine_nodate,
	title = {Machine {Learning} 4 {Support} {Vector} {Machine}},
	url = {https://kaggle.com/fengdanye/machine-learning-4-support-vector-machine},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from Bank Marketing},
	language = {en},
	urldate = {2020-12-06},
}

@misc{patel_chapter_2017,
	title = {Chapter 2 : {SVM} ({Support} {Vector} {Machine}) — {Theory}},
	shorttitle = {Chapter 2},
	url = {https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72},
	abstract = {Welcome to the second stepping stone of Supervised Machine Learning. Again, this chapter is divided into two parts. Part 1 (this one)…},
	language = {en},
	urldate = {2020-12-06},
	journal = {Medium},
	author = {Patel, Savan},
	month = may,
	year = {2017},
}

@misc{chen_support_2019,
	title = {Support {Vector} {Machine} — {Simply} {Explained}},
	url = {https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496},
	abstract = {The simplistic illustration of basic concepts in Support Vector Machine},
	language = {en},
	urldate = {2020-12-06},
	journal = {Medium},
	author = {Chen, Lujing},
	month = jan,
	year = {2019},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {9780387848570 9780387848587},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	urldate = {2020-12-06},
	publisher = {Springer New York},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
}

@misc{brownlee_understand_2019,
	title = {Understand the {Impact} of {Learning} {Rate} on {Neural} {Network} {Performance}},
	url = {https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/},
	abstract = {Deep learning neural networks are trained using the stochastic gradient descent optimization algorithm. The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a […]},
	language = {en-US},
	urldate = {2020-12-06},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jan,
	year = {2019},
}

@misc{noauthor_7_nodate,
	title = {7 {Types} of {Activation} {Functions} in {Neural} {Networks}: {How} to {Choose}?},
	shorttitle = {7 {Types} of {Activation} {Functions} in {Neural} {Networks}},
	url = {https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/},
	abstract = {Understand the evolution of different types of activation functions in neural network and learn the pros and cons of linear, step, ReLU, PRLeLU, Softmax and Swish.},
	language = {en-US},
	urldate = {2020-12-06},
	journal = {MissingLink.ai},
}

@misc{loy_how_2020,
	title = {How to build your own {Neural} {Network} from scratch in {Python}},
	url = {https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6},
	abstract = {A beginner’s guide to understanding the inner workings of Deep Learning},
	language = {en},
	urldate = {2020-12-06},
	journal = {Medium},
	author = {Loy, James},
	month = mar,
	year = {2020},
}
